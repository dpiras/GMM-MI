{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e52bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "from scipy import linalg\n",
    "import scipy.integrate as integrate\n",
    "from scipy.special import gamma\n",
    "import time\n",
    "\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.mixture._gaussian_mixture import _estimate_log_gaussian_prob, _compute_precision_cholesky, _estimate_gaussian_covariances_full\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import cluster\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e32b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_GMM(GMM):\n",
    "    \"\"\"\n",
    "    Custom GMM class based on the sklearn GMM class.\n",
    "    This allows to work with a GMM with fixed parameters, without fitting it.\n",
    "    It also allows to estimate MI with a certain number of MC samples.\n",
    "    The different initialisation types are dealt with separately.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_components=1,\n",
    "                 covariance_type=\"full\",\n",
    "                 tol=1e-5,\n",
    "                 reg_covar=1e-6,\n",
    "                 max_iter=100,\n",
    "                 n_init=1,\n",
    "                 init_params=\"random\",\n",
    "                 random_state=None,\n",
    "                 warm_start=False,\n",
    "                 verbose=0,\n",
    "                 verbose_interval=10,\n",
    "                 weights_init=None,\n",
    "                 means_init=None,\n",
    "                 precisions_init=None,\n",
    "                 covariances_init=None\n",
    "                 ):\n",
    "        super(my_GMM, self).__init__(n_components=n_components,\n",
    "                 covariance_type=covariance_type,\n",
    "                 tol=tol,\n",
    "                 reg_covar=reg_covar,\n",
    "                 max_iter=max_iter,\n",
    "                 n_init=n_init,\n",
    "                 init_params=init_params,\n",
    "                 random_state=random_state,\n",
    "                 warm_start=warm_start,\n",
    "                 verbose=verbose,\n",
    "                 verbose_interval=verbose_interval,\n",
    "                 weights_init=weights_init,\n",
    "                 means_init=means_init,\n",
    "                 precisions_init=precisions_init,\n",
    "                )\n",
    "\n",
    "        self.means_ = means_init\n",
    "        self.covariances_ = covariances_init\n",
    "        self.covariances_init = covariances_init\n",
    "        self.weights_ = weights_init\n",
    "        #self.random_state = random_state\n",
    "        #self.covariance_type = covariance_type\n",
    "        #self.precisions_cholesky_ = _compute_precision_cholesky(\n",
    "        #        self.covariances_, self.covariance_type\n",
    "        #    )\n",
    "\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the log-likelihood of each sample.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "        Returns\n",
    "        -------\n",
    "        log_prob : array, shape (n_samples,)\n",
    "            Log-likelihood of each sample in `X` under the current model.\n",
    "        \"\"\"\n",
    "        #check_is_fitted(self)\n",
    "        #X = self._validate_data(X, reset=False)\n",
    "\n",
    "        return logsumexp(self._estimate_weighted_log_prob(X), axis=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the labels for the data samples in X using trained model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Component labels.\n",
    "        \"\"\"\n",
    "        #check_is_fitted(self)\n",
    "        #X = self._validate_data(X, reset=False)\n",
    "        return self._estimate_weighted_log_prob(X).argmax(axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Evaluate the components' density for each sample.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "        Returns\n",
    "        -------\n",
    "        resp : array, shape (n_samples, n_components)\n",
    "            Density of each Gaussian component for each sample in X.\n",
    "        \"\"\"\n",
    "        # copied here to remove the fitting check\n",
    "        #check_is_fitted(self)\n",
    "        #X = self._validate_data(X, reset=False)\n",
    "        _, log_resp = self._estimate_log_prob_resp(X)\n",
    "        return np.exp(log_resp)\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        \"\"\"Generate random samples from the fitted Gaussian distribution.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int, default=1\n",
    "            Number of samples to generate.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Randomly generated sample.\n",
    "        y : array, shape (nsamples,)\n",
    "            Component labels.\n",
    "        \"\"\"\n",
    "        # copied here to remove the fitting check\n",
    "        # check_is_fitted(self)\n",
    "\n",
    "        if n_samples < 1:\n",
    "            raise ValueError(\n",
    "                \"Invalid value for 'n_samples': %d . The sampling requires at \"\n",
    "                \"least one sample.\" % (self.n_components)\n",
    "            )\n",
    "\n",
    "        _, n_features = self.means_.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples_comp = rng.multinomial(n_samples, self.weights_)\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            X = np.vstack(\n",
    "                [\n",
    "                    rng.multivariate_normal(mean, covariance, int(sample))\n",
    "                    for (mean, covariance, sample) in zip(\n",
    "                        self.means_, self.covariances_, n_samples_comp\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        elif self.covariance_type == \"tied\":\n",
    "            X = np.vstack(\n",
    "                [\n",
    "                    rng.multivariate_normal(mean, self.covariances_, int(sample))\n",
    "                    for (mean, sample) in zip(self.means_, n_samples_comp)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            X = np.vstack(\n",
    "                [\n",
    "                    mean + rng.randn(sample, n_features) * np.sqrt(covariance)\n",
    "                    for (mean, covariance, sample) in zip(\n",
    "                        self.means_, self.covariances_, n_samples_comp\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        y = np.concatenate(\n",
    "            [np.full(sample, j, dtype=int) for j, sample in enumerate(n_samples_comp)]\n",
    "        )\n",
    "\n",
    "        return (X, y)\n",
    "\n",
    "    def score_samples_marginal(self, X, index=0):\n",
    "        \"\"\"Compute the log-likelihood of each sample for the marginal model, indexed by either 0 (x) or 1 (y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "        index: integer\n",
    "            Either 0 (marginal x) or 1 (marginal y).\n",
    "        Returns\n",
    "        -------\n",
    "        log_prob : array, shape (n_samples,)\n",
    "            Log-likelihood of each sample in `X` under the current model.\n",
    "        \"\"\"\n",
    "\n",
    "        oned_cholesky = np.sqrt(1/self.covariances_[:, index, index]).reshape(-1, 1, 1)\n",
    "        marginal_logprob = _estimate_log_gaussian_prob(\n",
    "            X, self.means_[:, index].reshape(-1, 1), oned_cholesky, self.covariance_type\n",
    "        )\n",
    "\n",
    "        return logsumexp(np.log(self.weights_) + marginal_logprob, axis=1)\n",
    "\n",
    "\n",
    "    def estimate_MI_MC(self, MC_samples=100):\n",
    "        \"\"\"\n",
    "        Compute the mutual information (MI) associated with a particular GMM model, using MC integration\n",
    "        Parameters\n",
    "        ----------\n",
    "        MC_samples : integer\n",
    "            Number of Monte Carlo samples to perform numerical integration of the MI integral.\n",
    "        Returns\n",
    "        ----------\n",
    "        MI : integer\n",
    "            The value of mutual information.\n",
    "        -------\n",
    "        \"\"\"\n",
    "        # sample MC samples\n",
    "        points, clusters = self.sample(MC_samples)\n",
    "        \n",
    "        # we first evaluate the log-likelihood for the joint probability\n",
    "        joint = self.score_samples(points)\n",
    "\n",
    "        # we then evaluate the marginals; index=0 corresponds to x, index=y corresponds to y\n",
    "        marginal_x = self.score_samples_marginal(points[:, :1], index=0)\n",
    "        marginal_y = self.score_samples_marginal(points[:, 1:], index=1)\n",
    "\n",
    "        MI = np.mean(joint - marginal_x - marginal_y)\n",
    "        return MI\n",
    "    \n",
    "    def fit_predict(self, X, y=None):\n",
    "        \"\"\"Estimate model parameters using X and predict the labels for X.\n",
    "        The method fits the model n_init times and sets the parameters with\n",
    "        which the model has the largest likelihood or lower bound. Within each\n",
    "        trial, the method iterates between E-step and M-step for `max_iter`\n",
    "        times until the change of likelihood or lower bound is less than\n",
    "        `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is\n",
    "        raised. After fitting, it predicts the most probable label for the\n",
    "        input data points.\n",
    "        .. versionadded:: 0.20\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Component labels.\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._validate_data(X, dtype=[np.float64, np.float32], ensure_min_samples=2)\n",
    "        if X.shape[0] < self.n_components:\n",
    "            raise ValueError(\n",
    "                \"Expected n_samples >= n_components \"\n",
    "                f\"but got n_components = {self.n_components}, \"\n",
    "                f\"n_samples = {X.shape[0]}\"\n",
    "            )\n",
    "        self._check_initial_parameters(X)\n",
    "\n",
    "\n",
    "        # if we enable warm_start, we will have a unique initialisation\n",
    "        do_init = True#not (self.warm_start and hasattr(self, \"converged_\"))\n",
    "        n_init = 1#self.n_init if do_init else 1\n",
    "\n",
    "        max_lower_bound = -np.inf\n",
    "        self.converged_ = False\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "        for init in range(n_init):\n",
    "            self._print_verbose_msg_init_beg(init)\n",
    "            import time\n",
    "            inittt = time.time()\n",
    "            if do_init:\n",
    "                self._initialize_parameters(X, random_state)\n",
    "            print(time.time()-inittt)\n",
    "\n",
    "            lower_bound = -np.inf if do_init else self.lower_bound_\n",
    "\n",
    "            for n_iter in range(1, self.max_iter + 1):\n",
    "                #if n_iter==179:\n",
    "                #    try:\n",
    "                #        #print(n_iter)\n",
    "                #        print(np.linalg.eig(self.covariances_[2]))\n",
    "                #        #print(self.means_[2])\n",
    "                #        #ind = np.argsort(log_resp[:, 2])[-5:]\n",
    "                #        #print(X[ind])\n",
    "                #        #print(log_resp[np.argmax(log_resp[:, 2])])\n",
    "                #        #plt.hist(log_resp[:, 4])\n",
    "                #    except:\n",
    "                #        pass\n",
    "\n",
    "                prev_lower_bound = lower_bound\n",
    "\n",
    "                log_prob_norm, log_resp = self._e_step(X)\n",
    "                self._m_step(X, log_resp)\n",
    "                lower_bound = self._compute_lower_bound(log_resp, log_prob_norm)\n",
    "\n",
    "                change = lower_bound - prev_lower_bound\n",
    "                self._print_verbose_msg_iter_end(n_iter, change)\n",
    "\n",
    "                if abs(change) < self.tol:\n",
    "                    self.converged_ = True\n",
    "                    break\n",
    "\n",
    "            self._print_verbose_msg_init_end(lower_bound)\n",
    "\n",
    "            if lower_bound > max_lower_bound or max_lower_bound == -np.inf:\n",
    "                max_lower_bound = lower_bound\n",
    "                best_params = self._get_parameters()\n",
    "                best_n_iter = n_iter\n",
    "\n",
    "        if not self.converged_:\n",
    "            warnings.warn(\n",
    "                \"Initialization %d did not converge. \"\n",
    "                \"Try different init parameters, \"\n",
    "                \"or increase max_iter, tol \"\n",
    "                \"or check for degenerate data.\" % (init + 1),\n",
    "                ConvergenceWarning,\n",
    "            )\n",
    "\n",
    "        self._set_parameters(best_params)\n",
    "        self.n_iter_ = best_n_iter\n",
    "        self.lower_bound_ = max_lower_bound\n",
    "\n",
    "        # Always do a final e-step to guarantee that the labels returned by\n",
    "        # fit_predict(X) are always consistent with fit(X).predict(X)\n",
    "        # for any value of max_iter and tol (and any random_state).\n",
    "        _, log_resp = self._e_step(X)\n",
    "\n",
    "        return log_resp.argmax(axis=1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce4787a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we also focus on initialising the GMM parameters\n",
    "# we provide four different initialisation types, which return weights, means and covs\n",
    "# these will go as input into the GMM class, so that we can ignore whatever happens there\n",
    "\n",
    "  \n",
    "def initialize_parameters(X, random_state, n_components=1, s=None, reg_covar=1e-6, init_type='random'):\n",
    "    \"\"\"Initialize the model parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape  (n_samples, n_features)\n",
    "    random_state : RandomState\n",
    "        A random number generator instance that controls the random seed used for the method chosen to initialize the parameters.\n",
    "    n_components: int\n",
    "        Number of components of the GMM to fit.\n",
    "    s : float\n",
    "        If set, sets component variances in the 'random' and 'minmax' cases. \n",
    "        If s is not given, it will be set such that the volume of all components\n",
    "        completely fills the space covered by data.\n",
    "    init_type : {'random', 'minmax', 'kmeans', 'random_sklearn', 'kmeans_sklearn'}, default='random'\n",
    "        The method used to initialize the weights, the means and the\n",
    "        precisions.\n",
    "        Must be one of:\n",
    "            'random': weights are set uniformly, covariances are proprtional to identity (with prefactor s^2). \n",
    "            For each mean, a data sample is selected at random, and a multivariant Gaussian with variance s^2 offset is added.\n",
    "            'minmax': same as above, but means are distributed randomly over the range that is covered by data.\n",
    "            'kmeans': k-means clustering run as in Algorithm 1 from Bloemer & Bujna (arXiv:1312.5946), as implemented by Melchior & Goulding (arXiv:1611.05806)\n",
    "             WARNING: The result of this call are not deterministic even if rng is set because scipy.cluster.vq.kmeans2 uses its own initialization. \n",
    "             TO DO: require scipy > 1.7, and include \"seed=random_state\" in the kmeans call\n",
    "            'kmeans_sklearn' : responsibilities are initialized using kmeans.\n",
    "            'random_sklearn' : responsibilities are initialized randomly.\n",
    "    reg_covar : float\n",
    "        The regularization added to the diagonal of the covariance matrices.\n",
    "    Returns\n",
    "    ----------\n",
    "    weights : array, shape (n_components, 1)\n",
    "        The initial weights of the GMM model.\n",
    "    means : array, shape (n_components, n_features)\n",
    "        The initial means of the GMM model.        \n",
    "    covariances : array, shape (n_components, n_features, n_features)\n",
    "        The initial covariance matrices of the GMM model.        \n",
    "    \"\"\"\n",
    "    n_samples, n_dim = X.shape\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "    if s is None and (init_type=='random' or init_type=='minmax'):\n",
    "        min_pos = X.min(axis=0)\n",
    "        max_pos = X.max(axis=0)\n",
    "        vol_data = np.prod(max_pos-min_pos)\n",
    "        s = (vol_data / n_components * gamma(n_dim*0.5 + 1))**(1/n_dim) / np.sqrt(np.pi)\n",
    "        print(f\"Scale s set to s={s:.2f}...\")\n",
    "\n",
    "    if init_type == \"random\":\n",
    "\n",
    "        weights = np.repeat(1/n_components, n_components)\n",
    "        # initialize components around data points with uncertainty s\n",
    "        refs = random_state.randint(0, n_samples, size=n_components)\n",
    "\n",
    "        means = X[refs] + random_state.multivariate_normal(np.zeros(n_dim), s**2 * np.eye(n_dim), size=n_components)\n",
    "        \n",
    "        covariances = np.repeat(s**2 * np.eye(n_dim)[np.newaxis, :, :], n_components, axis=0)\n",
    "\n",
    "    elif init_type == \"minmax\":\n",
    "\n",
    "        weights = np.repeat(1/n_components, n_components)\n",
    "\n",
    "        min_pos = X.min(axis=0)\n",
    "        max_pos = X.max(axis=0)\n",
    "        means = min_pos + (max_pos-min_pos)*random_state.rand(n_components, n_dim)\n",
    "        \n",
    "        covariances = np.repeat(s**2 * np.eye(n_dim)[np.newaxis, :, :], n_components, axis=0)\n",
    "\n",
    "    elif init_type == 'kmeans':\n",
    "        from scipy.cluster.vq import kmeans2\n",
    "        center, label = kmeans2(X, n_components)\n",
    "        weights = np.zeros(n_components)\n",
    "        means = np.zeros((n_components, n_dim))\n",
    "        covariances = np.zeros((n_components, n_dim, n_dim))\n",
    "\n",
    "        for k in range(n_components):\n",
    "            mask = (label == k)\n",
    "            weights[k] = mask.sum() / len(X)\n",
    "            means[k,:] = X[mask].mean(axis=0)\n",
    "            d_m = X[mask] - means[k,:] \n",
    "            # funny way of saying: for each point i, do the outer product\n",
    "            # of d_m with its transpose and sum over i\n",
    "            covariances[k,:,:] = (d_m[:, :, None] * d_m[:, None, :]).sum(axis=0) / len(X)\n",
    "\n",
    "    elif init_type == \"random_sklearn\":\n",
    "        resp = random_state.rand(n_samples, n_components)\n",
    "        resp /= resp.sum(axis=1)[:, np.newaxis]\n",
    "        nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "        \n",
    "        weights = nk/n_samples\n",
    "        means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "        covariances = _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar)\n",
    "\n",
    "    elif init_type == \"kmeans_sklearn\":\n",
    "        resp = np.zeros((n_samples, n_components))\n",
    "        label = (\n",
    "            cluster.KMeans(\n",
    "                n_clusters=n_components, n_init=1, random_state=random_state\n",
    "            )\n",
    "            .fit(X)\n",
    "            .labels_\n",
    "        )\n",
    "        resp[np.arange(n_samples), label] = 1\n",
    "        nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "        \n",
    "        weights = nk/n_samples\n",
    "        means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "        covariances = _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar)\n",
    "\n",
    "    else:\n",
    "        # TO DO: raise error instead of just priting it\n",
    "        print(\"Error: initalisation type not specified or not known; it should be one of 'random', 'minmax', 'kmeans', 'random_sklearn', 'kmeans_sklearn'\")\n",
    "        \n",
    "    precisions = np.empty_like(covariances)\n",
    "    for i in range(n_components):\n",
    "        precisions[i] = np.linalg.inv(covariances[i])\n",
    "        \n",
    "    return weights, means, covariances, precisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83db3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI_procedure(X, n_components=1, n_folds=5, n_inits=5, init_type='random', n_bootstrap=100, MC_samples=1e5, reg_covar=1e-6, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Docstring TO DO\n",
    "    \"\"\"\n",
    "    initial_time = time.time()\n",
    "    # this will be used to store mean validation log-likelihood \n",
    "    val_scores_seeds = np.zeros(n_inits)\n",
    "\n",
    "    # prepare the folds; note the splitting will be the same for all initialisations\n",
    "    # the random seed is fixed here, but results should be independent of the exact split\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # fix the random seed first\n",
    "    for r in range(n_inits):\n",
    "\n",
    "        w_init, m_init, c_init, p_init = initialize_parameters(X, r, n_components=n_components, init_type=init_type)\n",
    "        validation_scores = []\n",
    "\n",
    "        for train_indices, valid_indices in kf.split(X):\n",
    "            X_training = X[train_indices]\n",
    "            X_validation = X[valid_indices]\n",
    "            \n",
    "            fitted_gmm = my_GMM(n_components=n_components, reg_covar=reg_covar, \n",
    "                            tol=tol, max_iter=10000, \n",
    "                            random_state=r, weights_init=w_init, \n",
    "                            means_init=m_init, precisions_init=p_init).fit(X_training)\n",
    "\n",
    "            # we take the mean logL per sample, since folds might have slightly different sizes\n",
    "            val_score = fitted_gmm.score_samples(X_validation).mean()\n",
    "            #print(val_score)\n",
    "            validation_scores.append(np.copy(val_score))\n",
    "\n",
    "        # take mean of current seed's val scores\n",
    "        val_scores_seeds[r] = np.mean(validation_scores)\n",
    "        #print()\n",
    "        \n",
    "    # select seed with highest val score\n",
    "    best_seed = np.argmax(val_scores_seeds)\n",
    "\n",
    "    w_init, m_init, c_init, p_init = initialize_parameters(X, best_seed, n_components=n_components, init_type=init_type)\n",
    "        \n",
    "    #best_fitted_GMM = my_GMM(n_components=n_components, reg_covar=reg_covar, \n",
    "    #                tol=tol, max_iter=10000, \n",
    "    #                random_state=best_seed, weights_init=w_init, \n",
    "    #                means_init=m_init, precisions_init=p_init).fit(X)\n",
    "    #MI_estimate = best_fitted_GMM.estimate_MI_MC(MC_samples=1e5)\n",
    "    \n",
    "    \n",
    "    # bootstrap samples and calculate MI each time\n",
    "    MI_estimates = np.zeros(n_bootstrap)\n",
    "\n",
    "    # bootstrap available samples\n",
    "    for i in range(n_bootstrap):\n",
    "        # we use i to change the seed so that the results will be fully reproducible\n",
    "        rng = np.random.default_rng(i)\n",
    "        X_bs = rng.choice(X, X.shape[0])\n",
    "        gmm = my_GMM(n_components=n_components, reg_covar=reg_covar, \n",
    "                    tol=tol, max_iter=10000, \n",
    "                    random_state=best_seed, weights_init=w_init, \n",
    "                    means_init=m_init, precisions_init=p_init).fit(X_bs)\n",
    "\n",
    "\n",
    "        current_MI_estimate = gmm.estimate_MI_MC(MC_samples=MC_samples)\n",
    "        MI_estimates[i] = current_MI_estimate #- true_MI_analytical\n",
    "\n",
    "    print(f'Total time to run with fixed number of components: {time.time()-initial_time:.2f} s')\n",
    "    # we return the entire sampling distribution, not just mean and variance\n",
    "    return MI_estimates#np.mean(MI_estimates), np.std(MI_estimates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098df8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI_procedure_diffconvergence(X, n_components=1, n_folds=5, n_inits=5, init_type='random', n_bootstrap=100, MC_samples=1e5, reg_covar=1e-6, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Docstring TO DO\n",
    "    \"\"\"\n",
    "    initial_time = time.time()\n",
    "    # this will be used to store mean validation log-likelihood \n",
    "    val_scores_seeds = np.zeros(n_inits)\n",
    "    train_scores_seeds = np.zeros(n_inits)\n",
    "\n",
    "    # prepare the folds; note the splitting will be the same for all initialisations\n",
    "    # the random seed is fixed here, but results should be independent of the exact split\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # fix the random seed first\n",
    "    for r in range(n_inits):\n",
    "\n",
    "        w_init, m_init, c_init, p_init = initialize_parameters(X, r, n_components=n_components, init_type=init_type)\n",
    "        validation_scores = []\n",
    "        training_scores = []\n",
    "        \n",
    "        for train_indices, valid_indices in kf.split(X):\n",
    "            X_training = X[train_indices]\n",
    "            X_validation = X[valid_indices]\n",
    "            \n",
    "            fitted_gmm = my_GMM(n_components=n_components, reg_covar=reg_covar, \n",
    "                            tol=tol, max_iter=10000, \n",
    "                            random_state=r, weights_init=w_init, \n",
    "                            means_init=m_init, precisions_init=p_init).fit(X_training)\n",
    "\n",
    "            # we take the mean logL per sample, since folds might have slightly different sizes\n",
    "            val_score = fitted_gmm.score_samples(X_validation).mean()\n",
    "            train_score = fitted_gmm.score_samples(X_training).mean()\n",
    "\n",
    "            #print(val_score)\n",
    "            validation_scores.append(np.copy(val_score))\n",
    "            training_scores.append(np.copy(train_score))\n",
    "\n",
    "\n",
    "        # take mean of current seed's val scores\n",
    "        val_scores_seeds[r] = np.mean(validation_scores)\n",
    "        train_scores_seeds[r] = np.mean(training_scores)\n",
    "\n",
    "        #print()\n",
    "        \n",
    "    # select seed with highest val score\n",
    "    best_seed = np.argmax(val_scores_seeds)\n",
    "    best_val_score = np.max(val_scores_seeds)\n",
    "    best_train_score = np.max(train_scores_seeds)\n",
    "    \n",
    "    return best_seed, best_val_score, best_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ff958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88208591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three different models, chosen based on the number of components\n",
    "n_components = 1\n",
    "\n",
    "# fix random state for stability across runs\n",
    "random_state = 13\n",
    "\n",
    "if n_components == 1:\n",
    "    weights = np.array([1.0])\n",
    "    mu0 = [-1, 1]\n",
    "    means = np.array([mu0])\n",
    "    C0 = [[1, 0.6], [0.6, 1]]\n",
    "    covariances = np.array([C0])\n",
    "    gmm_true = my_GMM(n_components=n_components, weights_init=weights, means_init=means, covariances_init=covariances, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014a94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_MI_analytical(covariances):\n",
    "    # need to provide covariance matrix with shape [1, 2, 2] \n",
    "    return -0.5*np.log(1-(covariances[0, 0, 1] / (np.sqrt(covariances[0, 0, 0]*covariances[0, 1, 1])))**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a991bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22314355131420974"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_analytical = calc_MI_analytical(covariances)#*np.log2(np.exp(1)) # to get from nats to bits\n",
    "MI_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c4d50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "n_neighbors = 3\n",
    "knn_estimates = []\n",
    "n_bootstrap = 20\n",
    "\n",
    "for r in range(n_bootstrap):\n",
    "    gmm_true = my_GMM(n_components=n_components, weights_init=weights, means_init=means, covariances_init=covariances, random_state=r)\n",
    "    X = gmm_true.sample(N)[0]\n",
    "    knn_estimates.append(mutual_info_regression(X[:, 0].reshape(-1, 1), X[:, 1], n_neighbors=n_neighbors))#*np.log2(np.exp(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bf8118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24690975487737993, 0.05819068003634168)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(knn_estimates), np.std(knn_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148e1279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to run the procedure: 0.04 s\n"
     ]
    }
   ],
   "source": [
    "# now take a single realisation and bootstrap it\n",
    "\n",
    "knn_estimates_bs = []\n",
    "\n",
    "gmm_true = my_GMM(n_components=n_components, weights_init=weights, means_init=means, covariances_init=covariances, random_state=13)\n",
    "X = gmm_true.sample(N)[0]\n",
    "\n",
    "single_estimate = mutual_info_regression(X[:, 0].reshape(-1, 1), X[:, 1], n_neighbors=n_neighbors)#*np.log2(np.exp(1))\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "# bootstrap available samples\n",
    "for i in range(n_bootstrap):\n",
    "    # we use i to change the seed so that the results will be fully reproducible\n",
    "    rng = np.random.default_rng(i)\n",
    "    X_bs = rng.choice(X, X.shape[0])\n",
    "    knn_estimates_bs.append(mutual_info_regression(X_bs[:, 0].reshape(-1, 1), X_bs[:, 1], n_neighbors=n_neighbors))#*np.log2(np.exp(1)))\n",
    "    \n",
    "print(f'Total time to run the procedure: {time.time()-initial_time:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93544868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21236857] 0.7688319756384484 0.1258895307702064\n"
     ]
    }
   ],
   "source": [
    "print(single_estimate, np.mean(knn_estimates_bs), np.std(knn_estimates_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53d5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772e71fd",
   "metadata": {},
   "source": [
    "### Now, let's apply our procedure and hope for the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e795b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013899803161621094\n",
      "0.0001583099365234375\n",
      "0.00016832351684570312\n",
      "0.0001583099365234375\n",
      "0.0001480579376220703\n",
      "0.0001049041748046875\n",
      "1 -2.501188427128883\n",
      "0.0001595020294189453\n",
      "0.00016450881958007812\n",
      "0.00012826919555664062\n",
      "0.0001327991485595703\n",
      "0.00012063980102539062\n",
      "0.00013971328735351562\n",
      "0.1826472282409668\n",
      "Convergence reached at 1 components\n",
      "0.00013947486877441406\n",
      "9.655952453613281e-05\n",
      "9.775161743164062e-05\n",
      "9.918212890625e-05\n",
      "9.751319885253906e-05\n",
      "9.679794311523438e-05\n",
      "9.799003601074219e-05\n",
      "9.72747802734375e-05\n",
      "9.632110595703125e-05\n",
      "9.655952453613281e-05\n",
      "9.584426879882812e-05\n",
      "9.679794311523438e-05\n",
      "9.632110595703125e-05\n",
      "9.655952453613281e-05\n",
      "9.799003601074219e-05\n",
      "9.679794311523438e-05\n",
      "9.72747802734375e-05\n",
      "9.655952453613281e-05\n",
      "9.799003601074219e-05\n",
      "9.703636169433594e-05\n",
      "1 0.2219686519214497 0.045405874700352525\n",
      "Total time to run the procedure: 0.29 s\n"
     ]
    }
   ],
   "source": [
    "# now we do this for many components, from 1 to 5\n",
    "n_inits = 3\n",
    "n_folds = 2\n",
    "init_type = 'random_sklearn'\n",
    "MC_samples = 1e4\n",
    "tol = 1e-5\n",
    "reg_covar = 1e-12\n",
    "components_range = 100\n",
    "patience = 1\n",
    "pat_counter = 0\n",
    "\n",
    "all_MI_estimates = np.zeros((components_range, n_bootstrap))\n",
    "\n",
    "best_val = -np.inf\n",
    "\n",
    "initial_time = time.time()\n",
    "for n_components in range(1, components_range+1):\n",
    "    init = time.time()\n",
    "    current_seed, current_val, _ = MI_procedure_diffconvergence(X, n_components=n_components, n_folds=n_folds, \n",
    "                                                       init_type=init_type, n_inits=n_inits, n_bootstrap=20, \n",
    "                                                       MC_samples=MC_samples, tol=tol, reg_covar=reg_covar)\n",
    "    \n",
    "    # check if convergence has been reached based on val score\n",
    "    if current_val > best_val:\n",
    "        best_val = current_val\n",
    "        best_seed = current_seed\n",
    "        print(n_components, best_val)\n",
    "    else:\n",
    "        # if val score has not increased, then we should stop and calculate MI with the previous parameters\n",
    "        pat_counter += 1\n",
    "        if pat_counter >= patience:\n",
    "            best_components = n_components-patience # note we retrieve the model based on patience!\n",
    "            print(time.time()-init)\n",
    "            print(f'Convergence reached at {best_components} components') \n",
    "            w_init, m_init, c_init, p_init = initialize_parameters(X, best_seed, n_components=best_components, init_type=init_type)\n",
    "            MI_estimates = np.zeros(n_bootstrap)\n",
    "\n",
    "            # bootstrap available samples\n",
    "            for i in range(n_bootstrap):\n",
    "                # we use i to change the seed so that the results will be fully reproducible\n",
    "                rng = np.random.default_rng(i)\n",
    "                X_bs = rng.choice(X, X.shape[0])\n",
    "                gmm = my_GMM(n_components=best_components, reg_covar=reg_covar, \n",
    "                            tol=tol, max_iter=10000, \n",
    "                            random_state=best_seed, weights_init=w_init, \n",
    "                            means_init=m_init, precisions_init=p_init).fit(X_bs)\n",
    "\n",
    "                current_MI_estimate = gmm.estimate_MI_MC(MC_samples=MC_samples)#*np.log2(np.exp(1)) # from nats to bits\n",
    "                MI_estimates[i] = current_MI_estimate\n",
    "\n",
    "            print(best_components, np.mean(MI_estimates), np.std(MI_estimates)) \n",
    "            break\n",
    "\n",
    "print(f'Total time to run the procedure: {time.time()-initial_time:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f5fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "312548d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dpiras/MI/mine-pytorch/mine\n"
     ]
    }
   ],
   "source": [
    "%cd ./mine-pytorch/mine/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ec7a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#import sys\n",
    "#sys.path.insert(0, './mine-pytorch/mine')\n",
    "\n",
    "from models.mine import MutualInformationEstimator\n",
    "from pytorch_lightning import Trainer\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = 1 if device=='cuda' else 0\n",
    "print(device)\n",
    "dim = 1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dim = 1\n",
    "N = X.shape[0]\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "loss_type = ['mine']  # mine, mine_biased, fdiv\n",
    "loss = loss_type[0]\n",
    "\n",
    "train_set, test_set = train_test_split(X, test_size=0.1, random_state=42)\n",
    "\n",
    "tensor_x = torch.Tensor(train_set[:, :1]) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(train_set[:, 1:]) # transform to torch tensor\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "train_loader = torch.utils.data.DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tensor_x = torch.Tensor(test_set[:, :1]) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(test_set[:, 1:]) # transform to torch tensor\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "test_loader = torch.utils.data.DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(\n",
    "#        MultivariateNormalDataset(N, dim, rho), batch_size=batch_size, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(\n",
    "#        MultivariateNormalDataset(N, dim, rho), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "kwargs = {\n",
    "    'lr': lr,\n",
    "    'batch_size': batch_size,\n",
    "    'train_loader': train_loader,\n",
    "    'test_loader': test_loader,\n",
    "    'alpha': 1.0\n",
    "}\n",
    "\n",
    "model = MutualInformationEstimator(\n",
    "    dim, dim, loss=loss, **kwargs).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1adf528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 13:16:36.391930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/openmpi/lib\n",
      "2022-06-13 13:16:36.391961: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 3/3 [00:00<00:00, 58.19batch/s, batch_idx=2, loss=-0.224, loss_tqdm=-.0433, mi=0.0433, v_num=369928]\n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 366.93batch/s]\n",
      "MINE 0.2762424051761627\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=epochs, early_stop_callback=False, gpus=num_gpus)\n",
    "trainer.fit(model)\n",
    "trainer.test()\n",
    "\n",
    "#print(\"True_mi {}\".format(true_mi))\n",
    "print(\"MINE {}\".format(model.avg_test_mi))#*np.log2(np.exp(1))))\n",
    "#results.append((rho, model.avg_test_mi, true_mi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00c916f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 48.76batch/s, batch_idx=2, loss=-0.214, loss_tqdm=-.153, mi=0.153, v_num=369928]    \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 183.76batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 69.74batch/s, batch_idx=2, loss=-0.192, loss_tqdm=-.0981, mi=0.0981, v_num=369928]  \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 211.11batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 36.83batch/s, batch_idx=2, loss=-0.186, loss_tqdm=-.15, mi=0.15, v_num=369928]      \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 156.27batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 47.48batch/s, batch_idx=2, loss=-0.177, loss_tqdm=-.0352, mi=0.0352, v_num=369928]  \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 146.52batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 51.79batch/s, batch_idx=2, loss=-0.222, loss_tqdm=0.0622, mi=-.0622, v_num=369928]  \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 261.91batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 38.30batch/s, batch_idx=2, loss=-0.286, loss_tqdm=-.301, mi=0.301, v_num=369928]      \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 211.25batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 46.43batch/s, batch_idx=2, loss=-0.189, loss_tqdm=-.183, mi=0.183, v_num=369928]  \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 96.57batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 63.91batch/s, batch_idx=2, loss=-0.300, loss_tqdm=-.317, mi=0.317, v_num=369928]      \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 199.81batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 47.06batch/s, batch_idx=2, loss=-0.224, loss_tqdm=-.338, mi=0.338, v_num=369928]   \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 192.86batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 60.35batch/s, batch_idx=2, loss=-0.148, loss_tqdm=-.0511, mi=0.0511, v_num=369928]  \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 442.23batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 49.10batch/s, batch_idx=2, loss=-0.290, loss_tqdm=-.228, mi=0.228, v_num=369928]    \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 280.07batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 63.40batch/s, batch_idx=2, loss=-0.207, loss_tqdm=-.213, mi=0.213, v_num=369928]    \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 321.20batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 64.90batch/s, batch_idx=2, loss=-0.292, loss_tqdm=-.108, mi=0.108, v_num=369928]    \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 259.88batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 57.73batch/s, batch_idx=2, loss=-0.313, loss_tqdm=-.529, mi=0.529, v_num=369928]   \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 419.05batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 55.30batch/s, batch_idx=2, loss=-0.233, loss_tqdm=-.327, mi=0.327, v_num=369928]    \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 226.45batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 60.64batch/s, batch_idx=2, loss=-0.245, loss_tqdm=-.537, mi=0.537, v_num=369928]   \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 412.72batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 58.99batch/s, batch_idx=2, loss=-0.214, loss_tqdm=-.272, mi=0.272, v_num=369928]      \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 355.25batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 61.87batch/s, batch_idx=2, loss=-0.237, loss_tqdm=-.207, mi=0.207, v_num=369928]   \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 342.71batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 65.01batch/s, batch_idx=2, loss=-0.226, loss_tqdm=-.139, mi=0.139, v_num=369928]    \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 406.42batch/s]\n",
      "Epoch 50: 100%|██████████| 3/3 [00:00<00:00, 66.32batch/s, batch_idx=2, loss=-0.204, loss_tqdm=-.108, mi=0.108, v_num=369928]    \n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 237.30batch/s]\n",
      "Total time to run the procedure: 55.80 s\n"
     ]
    }
   ],
   "source": [
    "# now take a single realisation and bootstrap it\n",
    "\n",
    "mine_estimates_bs = []\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "im = 1\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "loss_type = ['mine']  # mine, mine_biased, fdiv\n",
    "loss = loss_type[0]\n",
    "\n",
    "# bootstrap available samples\n",
    "for i in range(n_bootstrap):\n",
    "    # we use i to change the seed so that the results will be fully reproducible\n",
    "    rng = np.random.default_rng(i)\n",
    "    X_bs = rng.choice(X, X.shape[0])\n",
    "\n",
    "    N = X_bs.shape[0]\n",
    "    train_size = int(0.9 * N)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_set, test_set = train_test_split(X_bs, test_size=0.1, random_state=42)\n",
    "\n",
    "    tensor_x = torch.Tensor(train_set[:, :1]) # transform to torch tensor\n",
    "    tensor_y = torch.Tensor(train_set[:, 1:]) # transform to torch tensor\n",
    "    my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "    train_loader = torch.utils.data.DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    tensor_x = torch.Tensor(test_set[:, :1]) # transform to torch tensor\n",
    "    tensor_y = torch.Tensor(test_set[:, 1:]) # transform to torch tensor\n",
    "    my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "    test_loader = torch.utils.data.DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    #train_loader = torch.utils.data.DataLoader(\n",
    "    #        MultivariateNormalDataset(N, dim, rho), batch_size=batch_size, shuffle=True)\n",
    "    #test_loader = torch.utils.data.DataLoader(\n",
    "    #        MultivariateNormalDataset(N, dim, rho), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    kwargs = {\n",
    "        'lr': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'train_loader': train_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'alpha': 1.0\n",
    "    }\n",
    "\n",
    "    model = MutualInformationEstimator(\n",
    "        dim, dim, loss=loss, **kwargs).to(device)\n",
    "\n",
    "    \n",
    "    trainer = Trainer(max_epochs=epochs, early_stop_callback=False, gpus=num_gpus)\n",
    "    trainer.fit(model)\n",
    "    trainer.test()\n",
    "\n",
    "    mine_estimates_bs.append(model.avg_test_mi)#*np.log2(np.exp(1)))\n",
    "    #print(\"True_mi {}\".format(true_mi))\n",
    "    #print(\"MINE {}\".format(model.avg_test_mi*np.log2(np.exp(1))))\n",
    "    \n",
    "print(f'Total time to run the procedure: {time.time()-initial_time:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d9e6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2562943 0.068165384\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(mine_estimates_bs), np.std(mine_estimates_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc2a5383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26271856 0.065825626\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(mine_estimates_bs), np.std(mine_estimates_bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cba47",
   "metadata": {},
   "source": [
    "### Compare to a simple covariance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74ea52df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to run the procedure: 0.07 s\n"
     ]
    }
   ],
   "source": [
    "# now take a single realisation and bootstrap it\n",
    "\n",
    "true_estimates_bs = []\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "# bootstrap available samples\n",
    "for i in range(n_bootstrap):\n",
    "    # we use i to change the seed so that the results will be fully reproducible\n",
    "    gmm_true_ = my_GMM(n_components=n_components, weights_init=weights, means_init=means, covariances_init=covariances, random_state=i+100)\n",
    "    X_ = gmm_true_.sample(N)[0]\n",
    "    #cov_matrix = np.cov(X_, ddof=1, rowvar=False) # to remove bias\n",
    "    #true_estimates_bs.append(calc_MI_analytical(cov_matrix.reshape(-1, 2, 2)))\n",
    "    true_estimates_bs.append(mutual_info_regression(X_[:, 0].reshape(-1, 1), X_[:, 1], n_neighbors=2))#*np.log2(np.exp(1)))\n",
    "\n",
    "    \n",
    "print(f'Total time to run the procedure: {time.time()-initial_time:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7645177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24774525924200316 0.05571817731389326\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(true_estimates_bs), np.std(true_estimates_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd5c8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./bs_knn.npy', knn_estimates_bs)\n",
    "#np.save('./bs_mine.npy', mine_estimates_bs)\n",
    "#np.save('./bs_ours.npy', MI_estimates)\n",
    "#np.save('./bs_analytic.npy', MI_analytical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "389e009d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mine_estimates_bs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_207948/3990264890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'major'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_estimates_bs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_estimates_bs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'salmon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'KSG (0.06 s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmine_estimates_bs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmine_estimates_bs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'forestgreen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MINE (60 s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMI_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMI_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grey'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Ours (0.5 s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#ax.errorbar( np.mean(true_estimates_bs), 0.3, yerr=None, xerr=np.std(true_estimates_bs), fmt='.', markersize=20, color='grey', capsize=10, elinewidth=2, label='True')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mine_estimates_bs' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot all of this\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "np.mean(knn_estimates_bs), np.std(knn_estimates_bs)\n",
    "\n",
    "label = '$N=200$ samples to fit, 20 bootstrap realisations'\n",
    "ax.tick_params(axis='both', which='major', labelsize=25, size=10)\n",
    "ax.errorbar(np.mean(knn_estimates_bs), 0.6, xerr=np.std(knn_estimates_bs), yerr=None,  fmt='.', markersize=20, color='salmon', capsize=10, elinewidth=2, label='KSG (0.06 s)')\n",
    "ax.errorbar(np.mean(mine_estimates_bs), 0.5, xerr=np.std(mine_estimates_bs), yerr=None, fmt='.', markersize=20, color='forestgreen', capsize=10, elinewidth=2, label='MINE (60 s)')\n",
    "ax.errorbar( np.mean(MI_estimates), 0.4, yerr=None, xerr=np.std(MI_estimates), fmt='.', markersize=20, color='grey', capsize=10, elinewidth=2, label='Ours (0.5 s)')\n",
    "#ax.errorbar( np.mean(true_estimates_bs), 0.3, yerr=None, xerr=np.std(true_estimates_bs), fmt='.', markersize=20, color='grey', capsize=10, elinewidth=2, label='True')\n",
    "ax.axvline(MI_analytical, 0, linestyle='--', lw=3, color='k', label='Ground truth')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "i1, i2 = 0, 1\n",
    "handles[i1], handles[i2] = handles[i2], handles[i1] \n",
    "labels[i1], labels[i2] = labels[i2], labels[i1] \n",
    "\n",
    "i1, i2 = 1, 2\n",
    "handles[i1], handles[i2] = handles[i2], handles[i1] \n",
    "labels[i1], labels[i2] = labels[i2], labels[i1] \n",
    "\n",
    "i1, i2 = 2, 3\n",
    "handles[i1], handles[i2] = handles[i2], handles[i1] \n",
    "labels[i1], labels[i2] = labels[i2], labels[i1]\n",
    "\n",
    "#i1, i2 = 3, 4\n",
    "#handles[i1], handles[i2] = handles[i2], handles[i1] \n",
    "#labels[i1], labels[i2] = labels[i2], labels[i1]\n",
    "\n",
    "#ax.set_title(label, fontsize=30)\n",
    "ax.legend(handles, labels, fontsize=28, frameon=False, loc='lower right')\n",
    "        \n",
    "ax.axes.yaxis.set_ticks([])\n",
    "ax.set_ylim((0.38, 0.62))\n",
    "\n",
    "\n",
    "ax.set_xlabel('Mutual information [nats]', fontsize=28);\n",
    "#plt.savefig('../../figures/MI_bootstrap.pdf',  bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a1db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
